{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jeroaranda/naturalattention/blob/main/Atenci%C3%B3n.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kFr2dszbdF6A",
    "outputId": "074ab795-8593-4a04-948e-d216e3f256a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.18.7)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.25.5)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.19.0)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.12.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.9)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.10.0\n",
      "    Uninstalling fsspec-2024.10.0:\n",
      "      Successfully uninstalled fsspec-2024.10.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed datasets-3.2.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb transformers datasets torch tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "NpwQ1SVJv9V4"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config\n",
    "from datasets import load_dataset\n",
    "import wandb\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "fJuSEdDzkZSF"
   },
   "outputs": [],
   "source": [
    "## efficiency\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config\n",
    "from datasets import load_dataset\n",
    "import wandb\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import os\n",
    "from datetime import datetime\n",
    "class WikiTextDataset(Dataset):\n",
    "    def __init__(self, tokenizer, split='train', max_length=64):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        print(f\"Loading WikiText-2 dataset ({split} split)...\")\n",
    "        # Load only 300 examples\n",
    "        dataset = load_dataset('wikitext', 'wikitext-2-raw-v1', split=f'{split}[:300]')\n",
    "\n",
    "        # Process in chunks\n",
    "        self.chunks = []\n",
    "        current_chunk = []\n",
    "        current_length = 0\n",
    "\n",
    "        for text in tqdm(dataset['text'], desc=\"Processing text\"):\n",
    "            if not text.strip():\n",
    "                continue\n",
    "\n",
    "            # Tokenize each text separately\n",
    "            tokens = tokenizer.encode(text, truncation=True, max_length=max_length)\n",
    "\n",
    "            if current_length + len(tokens) > max_length:\n",
    "                if current_chunk:\n",
    "                    self.chunks.append(current_chunk)\n",
    "                current_chunk = tokens\n",
    "                current_length = len(tokens)\n",
    "            else:\n",
    "                current_chunk.extend(tokens)\n",
    "                current_length += len(tokens)\n",
    "\n",
    "            if current_length >= max_length:\n",
    "                self.chunks.append(current_chunk[:max_length])\n",
    "                current_chunk = []\n",
    "                current_length = 0\n",
    "\n",
    "        if current_chunk:\n",
    "            self.chunks.append(current_chunk)\n",
    "\n",
    "        print(f\"Created {len(self.chunks)} chunks of maximum length {max_length}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.chunks)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.chunks[idx]\n",
    "        if len(chunk) < self.max_length + 1:\n",
    "            chunk = chunk + [self.tokenizer.pad_token_id] * (self.max_length + 1 - len(chunk))\n",
    "        return torch.tensor(chunk[:self.max_length + 1])\n",
    "def train_epoch(model, optimizer, train_loader, device, model_type, global_step):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_perplexity = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f\"Training {model_type} model\"):\n",
    "        try:\n",
    "            input_ids = batch[:, :-1].to(device)\n",
    "            labels = batch[:, 1:].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            perplexity = torch.exp(loss).item()\n",
    "\n",
    "            # Log every batch\n",
    "            wandb.log({\n",
    "                \"step\": global_step,\n",
    "                f\"{model_type}/batch/loss\": loss.item(),\n",
    "                f\"{model_type}/batch/perplexity\": perplexity,\n",
    "                f\"{model_type}_loss\": loss.item(),  # Additional metrics for direct comparison\n",
    "                \"global_step\": global_step\n",
    "            })\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_perplexity += perplexity\n",
    "            num_batches += 1\n",
    "            global_step += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in batch: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    metrics = {\n",
    "        'loss': total_loss / num_batches,\n",
    "        'perplexity': total_perplexity / num_batches,\n",
    "    }\n",
    "\n",
    "    return metrics, global_step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "7FlltFpt9x7S"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class NaturalAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.n_head = config.n_head\n",
    "        self.hidden_size = config.n_embd\n",
    "        self.head_dim = self.hidden_size // self.n_head\n",
    "\n",
    "        # Linear projections\n",
    "        self.q_proj = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.k_proj = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.v_proj = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.out_proj = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "\n",
    "    def forward(self, hidden_states, layer_past=None, attention_mask=None, head_mask=None,\n",
    "                use_cache=False, output_attentions=False):\n",
    "        batch_size, seq_length, _ = hidden_states.size()\n",
    "\n",
    "        # Project Q, K, V\n",
    "        q = self.q_proj(hidden_states)\n",
    "        k = self.k_proj(hidden_states)\n",
    "        v = self.v_proj(hidden_states)\n",
    "\n",
    "        # Reshape for multi-head attention\n",
    "        q = q.view(batch_size, seq_length, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(batch_size, seq_length, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(batch_size, seq_length, self.n_head, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Handle layer past if provided\n",
    "        if layer_past is not None:\n",
    "            past_key, past_value = layer_past\n",
    "            k = torch.cat((past_key, k), dim=-2)\n",
    "            v = torch.cat((past_value, v), dim=-2)\n",
    "\n",
    "        present = (k, v) if use_cache else None\n",
    "\n",
    "        # Compute raw attention energies\n",
    "        attention_energies = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            attention_energies = attention_energies + attention_mask\n",
    "\n",
    "        # Store raw energies for optimization\n",
    "        self.last_attention_energies = attention_energies.detach()\n",
    "\n",
    "        # Regular attention computation\n",
    "        attention_probs = F.softmax(attention_energies, dim=-1)\n",
    "\n",
    "        # Apply head mask if provided\n",
    "        if head_mask is not None:\n",
    "            attention_probs = attention_probs * head_mask\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, v)\n",
    "\n",
    "        # Reshape output\n",
    "        context_layer = context_layer.transpose(1, 2).contiguous()\n",
    "        context_layer = context_layer.view(batch_size, seq_length, self.hidden_size)\n",
    "\n",
    "        # Project output\n",
    "        output = self.out_proj(context_layer)\n",
    "\n",
    "        outputs = (output, present)\n",
    "        if output_attentions:\n",
    "            outputs += (attention_probs,)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "class GPT2NaturalAttentionBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = NaturalAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(config.n_embd, 4 * config.n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        layer_past=None,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        use_cache=False,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.ln_1(hidden_states)\n",
    "        attn_outputs = self.attn(\n",
    "            hidden_states,\n",
    "            layer_past=layer_past,\n",
    "            attention_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "        )\n",
    "        attn_output = attn_outputs[0]  # output_attentions tuples have varying length\n",
    "        outputs = attn_outputs[1:]\n",
    "\n",
    "        hidden_states = residual + attn_output\n",
    "\n",
    "        # Store attention energies in parameters for optimizer\n",
    "        for p in self.parameters():\n",
    "            p._attention_energies = self.attn.last_attention_energies\n",
    "\n",
    "        # Feed-forward block\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.ln_2(hidden_states)\n",
    "        feed_forward_output = self.mlp(hidden_states)\n",
    "        hidden_states = residual + feed_forward_output\n",
    "\n",
    "        if use_cache:\n",
    "            outputs = (hidden_states,) + outputs\n",
    "        else:\n",
    "            outputs = (hidden_states,) + outputs[1:]\n",
    "\n",
    "        return outputs\n",
    "\n",
    "class AttentionInformedOptimizer(torch.optim.AdamW):\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0.01, energy_scale=0.1):\n",
    "        super().__init__(params, lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "        self.energy_scale = energy_scale\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "\n",
    "                # Get attention energies if available\n",
    "                if hasattr(p, '_attention_energies'):\n",
    "                    # Scale gradient based on attention energies\n",
    "                    energy_factor = torch.tanh(p._attention_energies.abs().mean() * self.energy_scale)\n",
    "                    p.grad.data *= (1.0 + energy_factor)\n",
    "\n",
    "        # Perform regular Adam update\n",
    "        return super().step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "AwM-eFyhtF4y"
   },
   "outputs": [],
   "source": [
    "def train_both_models(standard_model, natural_atention_model,\n",
    "                     standard_optimizer, natural_atention_optimizer,\n",
    "                     train_loader, device, global_step):\n",
    "    standard_model.train()\n",
    "    natural_atention_model.train()\n",
    "\n",
    "    total_standard_loss = 0.0\n",
    "    total_natural_atention_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f\"Training both models\"):\n",
    "        try:\n",
    "            # Debug prints for batch shape\n",
    "\n",
    "\n",
    "            input_ids = batch[:, :-1].to(device)\n",
    "            labels = batch[:, 1:].to(device)\n",
    "\n",
    "\n",
    "\n",
    "            # Train standard model\n",
    "            standard_optimizer.zero_grad()\n",
    "            standard_outputs = standard_model(input_ids, labels=labels)\n",
    "            standard_loss = standard_outputs.loss\n",
    "\n",
    "            standard_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(standard_model.parameters(), 1.0)\n",
    "            standard_optimizer.step()\n",
    "\n",
    "            # Train natural_atention model\n",
    "            natural_atention_optimizer.zero_grad()\n",
    "            natural_atention_outputs = natural_atention_model(input_ids, labels=labels)\n",
    "            natural_atention_loss = natural_atention_outputs.loss\n",
    "\n",
    "            natural_atention_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(natural_atention_model.parameters(), 1.0)\n",
    "            natural_atention_optimizer.step()\n",
    "\n",
    "            # Calculate perplexities\n",
    "            standard_perplexity = torch.exp(standard_loss).item()\n",
    "            natural_atention_perplexity = torch.exp(natural_atention_loss).item()\n",
    "\n",
    "            # Log with same step for both models\n",
    "            wandb.log({\n",
    "                \"step\": global_step,\n",
    "                \"standard/batch/loss\": standard_loss.item(),\n",
    "                \"standard/batch/perplexity\": standard_perplexity,\n",
    "                \"natural_atention/batch/loss\": natural_atention_loss.item(),\n",
    "                \"natural_atention/batch/perplexity\": natural_atention_perplexity,\n",
    "                \"global_step\": global_step\n",
    "            })\n",
    "\n",
    "            total_standard_loss += standard_loss.item()\n",
    "            total_natural_atention_loss += natural_atention_loss.item()\n",
    "            num_batches += 1\n",
    "            global_step += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in batch: {str(e)}\")\n",
    "            print(f\"Full error traceback:\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "\n",
    "    metrics = {\n",
    "        'standard_loss': total_standard_loss / num_batches,\n",
    "        'natural_atention_loss': total_natural_atention_loss / num_batches,\n",
    "    }\n",
    "\n",
    "    return metrics, global_step\n",
    "\n",
    "def train_models(config_dict):\n",
    "\n",
    "    run = wandb.init(project=\"enhancednaturalattention\", config=config_dict,reinit=True)\n",
    "\n",
    "    # Define metrics for better visualization\n",
    "    wandb.define_metric(\"step\")\n",
    "    wandb.define_metric(\"global_step\")\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    model_config = GPT2Config(\n",
    "        vocab_size=tokenizer.vocab_size,\n",
    "        n_positions=config_dict['max_length'],\n",
    "        n_ctx=config_dict['max_length'],\n",
    "        n_embd=config_dict['n_embd'],\n",
    "        n_layer=config_dict['n_layer'],\n",
    "        n_head=config_dict['n_head']\n",
    "    )\n",
    "\n",
    "    standard_model = GPT2LMHeadModel(model_config).to(device)\n",
    "    # Initialize model with natural attention\n",
    "    natural_atention_model = GPT2LMHeadModel(model_config)\n",
    "    for i, block in enumerate(natural_atention_model.transformer.h):\n",
    "        natural_atention_model.transformer.h[i] = GPT2NaturalAttentionBlock(model_config)\n",
    "\n",
    "    # Use the attention-informed optimizer\n",
    "    natural_atention_optimizer = AttentionInformedOptimizer(\n",
    "        natural_atention_model.parameters(),\n",
    "        lr=config_dict['learning_rate'],\n",
    "        energy_scale=0.1  # Adjust this to control attention influence\n",
    "    )\n",
    "\n",
    "    train_dataset = WikiTextDataset(\n",
    "        tokenizer,\n",
    "        split='train',\n",
    "        max_length=config_dict['max_length']\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config_dict['batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers=2,\n",
    "        pin_memory=False\n",
    "    )\n",
    "\n",
    "    standard_optimizer = torch.optim.AdamW(\n",
    "        standard_model.parameters(),\n",
    "        lr=config_dict['learning_rate'],\n",
    "        weight_decay=0.01\n",
    "    )\n",
    "\n",
    "    checkpoint_dir = os.path.join(\"checkpoints\", run.id)\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    global_step = 0\n",
    "    for epoch in range(config_dict['epochs']):\n",
    "        print(f\"\\nEpoch {epoch+1}/{config_dict['epochs']}\")\n",
    "\n",
    "        metrics, global_step = train_both_models(\n",
    "            standard_model, natural_atention_model,\n",
    "            standard_optimizer, natural_atention_optimizer,\n",
    "            train_loader, device, global_step\n",
    "        )\n",
    "\n",
    "        if (epoch + 1) % config_dict['save_every'] == 0:\n",
    "            for model_type, model, optimizer in [\n",
    "                (\"standard\", standard_model, standard_optimizer),\n",
    "                (\"natural_atention\", natural_atention_model, natural_atention_optimizer)\n",
    "            ]:\n",
    "                checkpoint_path = os.path.join(\n",
    "                    checkpoint_dir,\n",
    "                    f\"gpt2_{model_type}_epoch_{epoch+1}.pt\"\n",
    "                )\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'metrics': metrics,\n",
    "                }, checkpoint_path)\n",
    "\n",
    "    print(\"Training completed successfully!\")\n",
    "    run.finish()\n",
    "    return standard_model, natural_atention_model\n",
    "\n",
    "# Configuration with more epochs\n",
    "config_dict = {\n",
    "    'max_length': 32,\n",
    "    'batch_size': 4,\n",
    "    'n_embd': 64,\n",
    "    'n_layer': 2,\n",
    "    'n_head': 2,\n",
    "    'learning_rate': 1e-3,\n",
    "    'epochs': 10,     # Increased epochs\n",
    "    'save_every': 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "jqXMjnh9tGe0",
    "outputId": "53400b7d-d36e-4eea-d419-1083ccaa3113"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:t5we1pod) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">treasured-armadillo-93</strong> at: <a href='https://wandb.ai/jeroaranda-sincronia-ai/enhancednaturalattention/runs/t5we1pod' target=\"_blank\">https://wandb.ai/jeroaranda-sincronia-ai/enhancednaturalattention/runs/t5we1pod</a><br/> View project at: <a href='https://wandb.ai/jeroaranda-sincronia-ai/enhancednaturalattention' target=\"_blank\">https://wandb.ai/jeroaranda-sincronia-ai/enhancednaturalattention</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241211_182739-t5we1pod/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:t5we1pod). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20241211_182850-nhytknk3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jeroaranda-sincronia-ai/enhancednaturalattention/runs/nhytknk3' target=\"_blank\">summer-meadow-94</a></strong> to <a href='https://wandb.ai/jeroaranda-sincronia-ai/enhancednaturalattention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jeroaranda-sincronia-ai/enhancednaturalattention' target=\"_blank\">https://wandb.ai/jeroaranda-sincronia-ai/enhancednaturalattention</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jeroaranda-sincronia-ai/enhancednaturalattention/runs/nhytknk3' target=\"_blank\">https://wandb.ai/jeroaranda-sincronia-ai/enhancednaturalattention/runs/nhytknk3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading WikiText-2 dataset (train split)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing text: 100%|██████████| 300/300 [00:00<00:00, 1350.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 181 chunks of maximum length 32\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training both models: 100%|██████████| 46/46 [00:16<00:00,  2.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training both models: 100%|██████████| 46/46 [00:17<00:00,  2.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training both models: 100%|██████████| 46/46 [00:17<00:00,  2.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training both models: 100%|██████████| 46/46 [00:18<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training both models: 100%|██████████| 46/46 [00:16<00:00,  2.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training both models: 100%|██████████| 46/46 [00:16<00:00,  2.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training both models: 100%|██████████| 46/46 [00:17<00:00,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training both models: 100%|██████████| 46/46 [00:17<00:00,  2.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training both models: 100%|██████████| 46/46 [00:16<00:00,  2.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training both models: 100%|██████████| 46/46 [00:16<00:00,  2.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed successfully!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>global_step</td><td>▁▁▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇█</td></tr><tr><td>natural_atention/batch/loss</td><td>█▆▆▆▆▄▃▄▃▄▄▃▂▄▃▄▄▃▂▃▄▄▃▃▃▂▂▁▃▂▃▂▂▂▃▂▁▁▂▁</td></tr><tr><td>natural_atention/batch/perplexity</td><td>▇█▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>standard/batch/loss</td><td>▇█▇▆▅▅▅▄▃▃▄▃▅▃▄▄▃▄▂▂▄▃▃▄▁▂▂▂▂▁▃▃▄▄▃▃▁▄▂▃</td></tr><tr><td>standard/batch/perplexity</td><td>█▄▄▃▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>step</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>global_step</td><td>459</td></tr><tr><td>natural_atention/batch/loss</td><td>3.08678</td></tr><tr><td>natural_atention/batch/perplexity</td><td>21.90648</td></tr><tr><td>standard/batch/loss</td><td>3.95251</td></tr><tr><td>standard/batch/perplexity</td><td>52.0659</td></tr><tr><td>step</td><td>459</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">summer-meadow-94</strong> at: <a href='https://wandb.ai/jeroaranda-sincronia-ai/enhancednaturalattention/runs/nhytknk3' target=\"_blank\">https://wandb.ai/jeroaranda-sincronia-ai/enhancednaturalattention/runs/nhytknk3</a><br/> View project at: <a href='https://wandb.ai/jeroaranda-sincronia-ai/enhancednaturalattention' target=\"_blank\">https://wandb.ai/jeroaranda-sincronia-ai/enhancednaturalattention</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241211_182850-nhytknk3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "standard_model, naturall_model = train_models(config_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0lWBiIMP7k97"
   },
   "source": [
    "# Natural attention is natural gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "8jhVLVaCC1fw"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def analyze_attention_training(standard_model, natural_model, train_loader, epochs=5):\n",
    "    \"\"\"Collect metrics during training for both models\"\"\"\n",
    "    metrics = {\n",
    "        'epoch': [], 'batch': [], 'model_type': [],\n",
    "        'loss': [], 'perplexity': [],\n",
    "        'attention_energy_mean': [], 'attention_energy_std': [],\n",
    "        'gradient_norm': []\n",
    "    }\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, batch in enumerate(tqdm(train_loader)):\n",
    "            # Get attention energies from both models\n",
    "            with torch.no_grad():\n",
    "                # Standard model metrics\n",
    "                std_outputs = standard_model(batch)\n",
    "                std_attention = standard_model.transformer.h[0].attn.last_attention_energies\n",
    "                std_grad_norm = torch.norm(torch.stack([p.grad.norm() for p in standard_model.parameters() if p.grad is not None]))\n",
    "\n",
    "                metrics['epoch'].append(epoch)\n",
    "                metrics['batch'].append(batch_idx)\n",
    "                metrics['model_type'].append('standard')\n",
    "                metrics['loss'].append(std_outputs.loss.item())\n",
    "                metrics['perplexity'].append(torch.exp(std_outputs.loss).item())\n",
    "                metrics['attention_energy_mean'].append(std_attention.mean().item())\n",
    "                metrics['attention_energy_std'].append(std_attention.std().item())\n",
    "                metrics['gradient_norm'].append(std_grad_norm.item())\n",
    "\n",
    "                # Natural attention model metrics\n",
    "                nat_outputs = natural_model(batch)\n",
    "                nat_attention = natural_model.transformer.h[0].attn.last_attention_energies\n",
    "                nat_grad_norm = torch.norm(torch.stack([p.grad.norm() for p in natural_model.parameters() if p.grad is not None]))\n",
    "\n",
    "                metrics['epoch'].append(epoch)\n",
    "                metrics['batch'].append(batch_idx)\n",
    "                metrics['model_type'].append('natural')\n",
    "                metrics['loss'].append(nat_outputs.loss.item())\n",
    "                metrics['perplexity'].append(torch.exp(nat_outputs.loss).item())\n",
    "                metrics['attention_energy_mean'].append(nat_attention.mean().item())\n",
    "                metrics['attention_energy_std'].append(nat_attention.std().item())\n",
    "                metrics['gradient_norm'].append(nat_grad_norm.item())\n",
    "\n",
    "    return pd.DataFrame(metrics)\n",
    "\n",
    "def plot_training_metrics(metrics_df):\n",
    "    \"\"\"Create a suite of plots comparing model performance\"\"\"\n",
    "    plt.style.use('seaborn')\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "    # 1. Loss curves\n",
    "    sns.lineplot(\n",
    "        data=metrics_df, x='epoch', y='loss', hue='model_type',\n",
    "        ax=axes[0,0], errorbar='sd'\n",
    "    )\n",
    "    axes[0,0].set_title('Training Loss')\n",
    "\n",
    "    # 2. Attention energy distribution\n",
    "    sns.boxplot(\n",
    "        data=metrics_df, x='epoch', y='attention_energy_mean',\n",
    "        hue='model_type', ax=axes[0,1]\n",
    "    )\n",
    "    axes[0,1].set_title('Attention Energy Distribution')\n",
    "\n",
    "    # 3. Gradient norm evolution\n",
    "    sns.lineplot(\n",
    "        data=metrics_df, x='epoch', y='gradient_norm',\n",
    "        hue='model_type', ax=axes[1,0]\n",
    "    )\n",
    "    axes[1,0].set_title('Gradient Norm Evolution')\n",
    "\n",
    "    # 4. Perplexity comparison\n",
    "    sns.violinplot(\n",
    "        data=metrics_df, x='epoch', y='perplexity',\n",
    "        hue='model_type', ax=axes[1,1], split=True\n",
    "    )\n",
    "    axes[1,1].set_title('Perplexity Distribution')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def analyze_attention_patterns(model, test_loader):\n",
    "    \"\"\"Analyze attention pattern stability and structure\"\"\"\n",
    "    attention_patterns = []\n",
    "\n",
    "    for batch in test_loader:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(batch)\n",
    "            attention = model.transformer.h[0].attn.last_attention_energies\n",
    "            attention_patterns.append(attention.cpu().numpy())\n",
    "\n",
    "    patterns = np.stack(attention_patterns)\n",
    "\n",
    "    # Plot attention pattern analysis\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "    # 1. Average attention pattern\n",
    "    sns.heatmap(\n",
    "        patterns.mean(axis=0).mean(axis=0),\n",
    "        ax=axes[0], cmap='viridis'\n",
    "    )\n",
    "    axes[0].set_title('Average Attention Pattern')\n",
    "\n",
    "    # 2. Attention stability across samples\n",
    "    stability = patterns.std(axis=0).mean(axis=0)\n",
    "    sns.heatmap(stability, ax=axes[1], cmap='rocket')\n",
    "    axes[1].set_title('Attention Pattern Stability')\n",
    "\n",
    "    # 3. Attention sparsity distribution\n",
    "    sparsity = (patterns > patterns.mean() + patterns.std()).mean(axis=(0,1))\n",
    "    sns.barplot(x=range(len(sparsity)), y=sparsity, ax=axes[2])\n",
    "    axes[2].set_title('Attention Sparsity by Position')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Example usage\n",
    "def run_analysis(standard_model, natural_model, train_loader, test_loader):\n",
    "    # Collect training metrics\n",
    "    metrics_df = analyze_attention_training(standard_model, natural_model, train_loader)\n",
    "\n",
    "    # Generate plots\n",
    "    training_fig = plot_training_metrics(metrics_df)\n",
    "    standard_patterns_fig = analyze_attention_patterns(standard_model, test_loader)\n",
    "    natural_patterns_fig = analyze_attention_patterns(natural_model, test_loader)\n",
    "\n",
    "    return {\n",
    "        'metrics': metrics_df,\n",
    "        'training_plot': training_fig,\n",
    "        'standard_patterns': standard_patterns_fig,\n",
    "        'natural_patterns': natural_patterns_fig\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 216
    },
    "id": "2y--TRV5_-ws",
    "outputId": "d683f6d3-45a9-462a-9cf8-52b33d0fbdad"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-418dbae56b67>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# After training both models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m test_loader = DataLoader(\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mWikiTextDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'max_length'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# After training both models\n",
    "test_loader = DataLoader(\n",
    "    WikiTextDataset(tokenizer, split='test', max_length=config_dict['max_length']),\n",
    "    batch_size=config_dict['batch_size'],\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "results = run_analysis(standard_model, natural_model, train_loader, test_loader)\n",
    "\n",
    "# Save the plots\n",
    "results['training_plot'].savefig('training_comparison.png')\n",
    "results['standard_patterns'].savefig('standard_attention_patterns.png')\n",
    "results['natural_patterns'].savefig('natural_attention_patterns.png')\n",
    "\n",
    "# You can also export metrics to CSV\n",
    "results['metrics'].to_csv('training_metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kpog09ic___V"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPsoO5ntqA5mAFGa/VlGzUk",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
